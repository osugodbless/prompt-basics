## Objective: Understand how parameters affect output diversity.

```
When we talk about parameters affecting output diversity, weâ€™re usually referring to AI model parameters like:

Temperature

Top-p (nucleus sampling)

Top-k

(Sometimes) frequency / presence penalties

These parameters control how creative vs predictable the output is.

Letâ€™s break it down clearly.

1ï¸âƒ£ Temperature â€” Controls Randomness
What it does:

Temperature scales how â€œconfidentâ€ the model is when choosing the next word.

Low temperature (0â€“0.3) â†’ More predictable, focused

Medium (0.5â€“0.8) â†’ Balanced

High (0.9â€“1.5) â†’ More random, creative, diverse

Example

Prompt:

â€œThe sky isâ€¦â€

Low temperature (0.2):

The sky is blue.

Medium (0.7):

The sky is blue with streaks of golden clouds at sunset.

High (1.2):

The sky is a melting ocean of light, whispering colors across the horizon.

Higher temperature = more output diversity.

But too high = incoherent or unstable output.

2ï¸âƒ£ Top-k â€” Limits Word Choices

Top-k tells the model:

â€œOnly choose from the top K most likely next words.â€

Low k (e.g., 10) â†’ Safer, less diverse

High k (e.g., 100) â†’ More diverse

If k is small, the model ignores rare words.
If k is large, it considers more unusual possibilities.

3ï¸âƒ£ Top-p (Nucleus Sampling) â€” Probability Threshold

Instead of picking the top K words, top-p says:

â€œChoose from the smallest set of words whose total probability adds up to p.â€

Low p (0.5) â†’ Very focused

High p (0.9â€“0.95) â†’ More diverse

Top-p is usually smoother and more natural than top-k.

4ï¸âƒ£ Frequency & Presence Penalties â€” Reduce Repetition

These increase diversity by discouraging repetition.

Frequency penalty â†’ Penalizes repeated words

Presence penalty â†’ Encourages introducing new topics

Without penalties:

AI might repeat phrases.

With penalties:

AI explores new wording and ideas.

ğŸ”¬ Why This Matters

In practical use:

Use Case	Preferred Settings
Code generation	Low temperature
Math problems	Low temperature
Creative writing	Higher temperature
Brainstorming	Mediumâ€“High
Legal/medical info	Low temperature

You donâ€™t want creativity in code correctness.
You do want diversity in storytelling.

ğŸ§  Conceptual Understanding (Important)

The model always predicts the next most probable word.

Parameters change:

How strictly it follows the highest probability

How much randomness is allowed

How wide the selection pool is

So:

Higher randomness â†’ higher output diversity

Lower randomness â†’ safer, more deterministic output

âš–ï¸ The Tradeoff

More diversity increases:

Creativity

Novelty

Variation

But also increases:

Risk of hallucination

Inconsistency

Loss of factual precision
```

## Use the same prompt with temperature = 0.2 and then with temperature = 0.9.

**temperature = 0.2**

```
Great â€” letâ€™s zoom in specifically on:

How does temperature = 0.2 affect output diversity?

ğŸ”¥ What Temperature Actually Does (Quick Reminder)

Temperature controls how much randomness the model allows when choosing the next word.

Mathematically, it reshapes the probability distribution before sampling.

Lower temperature (< 0.5) â†’ Sharp, peaked distribution

Higher temperature (> 0.8) â†’ Flatter distribution

ğŸ“‰ What Happens at Temperature = 0.2?

At 0.2, the model becomes very conservative.

It strongly favors the most probable next word.

Effect on Diversity:

âœ… Very low variation
âœ… Very consistent phrasing
âœ… Deterministic-style responses
âŒ Less creativity
âŒ Less exploration
âŒ More repetition across multiple runs

ğŸ§  Intuitive Explanation

Imagine the model thinks the next word probabilities look like this:

Word	Original Probability
blue	0.50
clear	0.30
vast	0.15
infinite	0.05

When temperature = 0.2, the highest probability word ("blue") becomes even more dominant, while lower ones shrink drastically.

So the model almost always picks:

blue

Rarely:

clear

Almost never:

vast or infinite

That means:

The same prompt will give nearly the same answer every time.

âœï¸ Example Comparison

Prompt:

"Describe the ocean."

Temperature = 0.2

The ocean is vast and blue, covering most of the Earth's surface.

Temperature = 1.0

The ocean is a restless mirror of the sky, breathing in tides and whispering ancient secrets.

Notice how 0.2 sticks to safe, textbook-like phrasing.

If T = 0.2, then 1/T = 5, meaning probabilities are raised to the 5th power.

That dramatically exaggerates the highest probability tokens and suppresses others.

So:

Distribution becomes very sharp.

Entropy decreases.

Diversity drops.

ğŸ¯ When Temperature = 0.2 Is Good

âœ” Code generation
âœ” Math solutions
âœ” Legal explanations
âœ” Medical content
âœ” Deterministic APIs
âœ” Factual Q&A

Because consistency > creativity.

âš ï¸ Downsides

Responses may feel repetitive.

Brainstorming becomes limited.

Creative tasks suffer.

Model may reuse identical structures.

ğŸ§  Big Picture

At temperature = 0.2:

The model behaves almost greedily â€” close to always picking the most likely next word.

So output diversity is very low.
```

**temperature = 0.9**

```
Temperature = 0.9

Temperature slightly less than 1 has the effect of slightly reducing randomness compared to the natural probability distribution.

Words with higher probability are slightly more likely to be chosen.

Words with lower probability are slightly less likely to appear.

So the output diversity is slightly reduced compared to temperature = 1.

ğŸ”¹ Intuitive Explanation

Think of temperature as a volume knob for randomness:

Temp = 1.0 â†’ normal volume â†’ moderate diversity

Temp = 0.9 â†’ just below normal â†’ a little less diversity

Temp = 0 â†’ deterministic â†’ almost no diversity

At 0.9:

The model still produces creative or varied outputs

But it slightly leans toward more probable words, improving coherence.

Rare or unusual outputs are less likely than at temp = 1.

ğŸ”¹ Visual Analogy

If you imagine a probability distribution of words:

Temperature = 1 â†’ use it exactly as is

Temperature = 0.9 â†’ distribution gets slightly â€œsharpenedâ€ â†’ top words become a bit more dominant

This reduces the chance of odd or unexpected words, but not by much.
```

## Repeat with different top-p values (e.g., 0.5 vs 1).

**top-p = 0.5**
```
What is top-p?

Top-p sets a cumulative probability threshold for choosing the next word.

The model only considers the smallest set of words whose combined probability â‰¥ p.

Words outside this set are ignored completely, no matter their original probability.

So top-p is like saying:

â€œOnly pick from the top words that together make up X% of total probability.â€

Top-p = 0.5

The model will only consider the most likely words whose total probability adds up to 50%.

Less likely words outside that 50% cannot appear.

This makes the output more conservative, because rare/unlikely words are excluded entirely.

ğŸ”¹ Intuitive Explanation

Smaller p (e.g., 0.3) â†’ very few words considered â†’ low diversity, more deterministic

Moderate p (0.5) â†’ some diversity, but still controlled â†’ moderate creativity

Larger p (e.g., 0.9 or 1.0) â†’ almost all words considered â†’ high diversity, more randomness

At top-p = 0.5, the model is biased toward high-probability words, so outputs are coherent and safe, but still allow some variation.

ğŸ”¹ Visual Analogy

Imagine a bag of words with probabilities:

Word	Probability
â€œtheâ€	0.2
â€œaâ€	0.15
â€œheâ€	0.1
â€œsheâ€	0.08
â€¦	â€¦

Cumulative probability 0.5 â†’ only include â€œthe + a + heâ€ â†’ pick randomly among these

Words like â€œsheâ€ or rarer words are ignored.
```

**top-p = 1**
```
Top-p (nucleus sampling) reminder

Top-p controls which words the model is allowed to pick based on cumulative probability.

The model only considers words in the top cumulative probability p, ignoring the rest.

Top-p = 1

All words are considered because cumulative probability 1 = 100% of the distribution.

No words are excluded, even very unlikely ones.

This maximizes potential diversity for the given temperature.

ğŸ”¹ Effect on Output Diversity

At top-p = 1:

Diversity depends mostly on temperature, because top-p no longer restricts anything.

Rare or unlikely words can appear, so the output can be very creative or random if temperature is high.

If temperature is low, outputs are still more predictable, even though all words are available.

ğŸ”¹ Intuitive Analogy

Think of top-p as a filter on a word bag:

Top-p = 0.5 â†’ only use the top half of the bag

Top-p = 1 â†’ use the entire bag â†’ every word is allowed

So top-p = 1 removes the restriction entirely.
```

## Record how the style, randomness, and focus of responses change.

For temperature = 0.2, the response style was repetitive. It wasn't random and focus was very high.

For temperature = 0.9, the response style was a lot more unpredictable, which made it moderately random.

For top-p = 0.5, the response style used mainly words that are well known and a little bit creative.

For top-p = 1, the response style used some rare words.